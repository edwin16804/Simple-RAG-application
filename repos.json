[
    {
        "repo_name": "Backend-Projects",
        "repo_link": "https://github.com/edwin16804/Backend-Projects",
        "readme": "Backend-Projects Expense Tracker Manage expenses through a CLI Expense Tracker by passing the details as CLI arguments Usage : python expense-tracker.py <argument> Commands: python expense_tracker.py add --description \"Lunch\" --amount 20 python expense_tracker.py add --description \"Dinner\" --amount 10 python expense_tracker.py list python expense_tracker.py summary python expense_tracker.py delete --id 2 python expense_tracker.py summary --month 8 python expense_tracker.py update --id 1 --amount 25 project URL : https://roadmap.sh/projects/expense-tracker Weather API with FastAPI + Redis Caching This project is a FastAPI backend that fetches weather data from the Visual Crossing API and uses Redis to cache responses, reducing API calls and improving performance. Features - Fetch weather data for a given city. - Cache results in Redis for 1 hour to avoid unnecessary API calls. - Simple and fast REST API built with FastAPI . - .env support for secure API key management. Run command: uvicorn weather-api:app --reload GitHub User Activity Using GitHub API to fetch user activity and display it in the terminal. Run command: python github-events.py Project URL : https://roadmap.sh/projects/github-user-activity"
    },
    {
        "repo_name": "Curvetopia--Shape-Regularization-Symmetry-Detection-and-Image-Contour-Processing",
        "repo_link": "https://github.com/edwin16804/Curvetopia--Shape-Regularization-Symmetry-Detection-and-Image-Contour-Processing",
        "readme": "Curvetopia--Shape-Regularization-Symmetry-Detection-and-Image-Contour-Processing Project Description: Shape Regularization, Symmetry Detection, and Contour-Based Image Processing This project involves developing a comprehensive Python-based tool for processing geometric shapes, both from CSV files and images. The tool reads path data from CSV files or processes shapes directly from images, visualizes the paths, regularizes the shapes to conform to ideal geometric forms (such as straight lines, circles, and rectangles), and then detects and reports symmetry within these shapes. Additionally, it includes functionality for contour detection and shape filling in images. The output includes visual plots of the original and regularized shapes, files saved in SVG and PNG formats, and processed images with filled contours. Overview Shapes represented as sequences of points or within images may not always perfectly align with ideal geometric forms due to noise, imperfections in data collection, or other factors. This project aims to regularize these shapes, meaning it transforms them into their ideal forms while maintaining their general structure. The tool then analyzes these regularized shapes to detect any inherent symmetries, such as symmetry along the x-axis, y-axis, or diagonal lines. For image data, the project includes an additional step of detecting and filling contours, which can be useful for visualizing and analyzing the structural components of the shapes. Functionality CSV File Reading : The tool reads a CSV file where each row represents a point in a path, with paths grouped by a unique identifier in the first column. The read_csv function organizes these points into paths based on the unique identifiers, making it easier to process them as distinct shapes. Image Processing with OpenCV : For images, the tool includes a process that uses OpenCV to detect and fill shapes: Loading and Grayscaling : The image is loaded from a PNG file and converted to grayscale to simplify the analysis. Edge Detection : Gaussian blur is applied to reduce noise, followed by Canny edge detection to identify the edges within the image. Contour Detection and Filling : The contours of the shapes are detected from the edges, and these contours are filled with a color (e.g., green) to highlight the shapes in the image. The processed image is then saved and displayed. Visualization : The plot function provides a visual representation of the shapes as they are initially read from the CSV, while cv2_imshow displays the original and processed images, allowing for a direct comparison of the before and after states. Shape Regularization : The regularize_shape function applies geometric rules to convert irregular point sequences into idealized shapes. Straight Line Detection : If a shape is identified as nearly straight, it is reduced to a line segment between the first and last points. Circle Detection : For shapes approximating a circle, the tool calculates the average radius and repositions the points evenly around the center. Rectangle Detection : Shapes resembling rectangles are adjusted to form a perfect rectangle with aligned edges. Symmetry Detection : The tool checks for symmetry in regularized shapes using the find_symmetry function. It assesses symmetry across various axes and diagonal lines and outputs whether the shape is symmetric about the x-axis, y-axis, or along lines such as y=x or y=-x. SVG and PNG Output : The polylines2svg function converts the regularized shapes into SVG format, a vector graphics format that allows for scalable and detailed visualization. Additionally, the tool saves the output as a PNG image, ensuring compatibility with a wide range of applications. Complete Workflow : The process_file function integrates all these steps, from reading the CSV to producing the final outputs. It also prints the detected symmetries to the console, providing immediate feedback on the analysis. Applications This tool is versatile and can be used in various fields requiring geometric analysis, such as computer graphics, architectural design, and pattern recognition. The inclusion of image processing capabilities extends its utility to scenarios where shapes are embedded within images, allowing for contour detection and filling, which are essential in image segmentation tasks. Conclusion This project offers a robust solution for reading, regularizing, visualizing, analyzing geometric shapes from CSV files, and processing shapes directly from images. Through its multifaceted functionality, it ensures that even imperfectly defined shapes can be regularized, analyzed for symmetry, and visualized in both vector and raster formats, providing clear and accurate geometric insights."
    },
    {
        "repo_name": "DL-based-IoT-intrusion-detection",
        "repo_link": "https://github.com/edwin16804/DL-based-IoT-intrusion-detection",
        "readme": "IoT Intrusion Detection Using Machine Learning & Deep Learning Project Overview This project focuses on intrusion detection in IoT devices using machine learning and deep learning models . With the increasing number of IoT devices, cybersecurity threats are becoming more sophisticated. This study aims to build an efficient Intrusion Detection System (IDS) that can classify various types of network attacks using the CICIOT2023 dataset . Models Used We implemented and compared the following models: Deep Learning Models LSTM (Long Short-Term Memory) \u2013 Best for sequential data (Achieved 99.89% accuracy ). GRU (Gated Recurrent Unit) \u2013 Similar to LSTM but computationally more efficient ( 98.69% accuracy ). Deep CNN (Convolutional Neural Network) \u2013 Extracts spatial patterns from network traffic ( 98.34% accuracy ). Machine Learning Models Decision Tree \u2013 Simple yet effective rule-based classification ( 99.37% accuracy ). XGBoost (Extreme Gradient Boosting) \u2013 Fast and accurate boosting algorithm ( 99.30% accuracy ). Dataset We used the CICIOT2023 dataset , which consists of: - 105 IoT devices (Smart home, Zigbee, Z-Wave, etc.) - 33 different attack types (DDoS, DoS, Brute Force, Mirai, etc.) - 47 features extracted from network traffic logs. Preprocessing Steps Handling Missing Values Feature Selection Using Correlation Analysis Label Encoding for Categorical Features Standardization Using StandardScaler Splitting Dataset (70% Train, 30% Test) Model Training & Evaluation Each model was trained and evaluated using the following metrics: Accuracy Precision, Recall, F1-score False Positive Rate (FPR) | Model | Accuracy | Precision | Recall | F1-score | |------------|----------|-----------|--------|----------| | LSTM | 99.89% | 99.72% | 99.85% | 99.78% | | GRU | 98.69% | 98.41% | 98.54% | 98.47% | | Deep CNN | 98.34% | 98.58% | 97.00% | 97.20% | | Decision Tree | 99.37% | 99.00% | 99.10% | 99.05% | | XGBoost | 99.30% | 0.85 (macro avg) | 0.73 (macro avg) | 0.75 (macro avg) | Key Findings LSTM performed best , achieving 99.89% accuracy , but required longer training time. GRU and CNN models also showed strong performance with high accuracy and stability. Decision Trees & XGBoost were faster and still highly accurate, making them suitable for real-time detection. Class imbalance affected recall for rare attack types (e.g., Backdoor Malware, Uploading Attack)."
    },
    {
        "repo_name": "Finance-Tracker",
        "repo_link": "https://github.com/edwin16804/Finance-Tracker",
        "readme": "Personal Finance Tracker Developed a Personal Finance Manager to help users effectively manage their finances by organizing and optimizing income, expenses, savings, and investments. Key Features: Income Tracking: Allows users to record and monitor all sources of income. Expense Management: Enables users to categorize and track all expenses. Savings Optimization: Provides tools and insights to help users save more effectively. Investment Tracking: Monitors investments and provides performance analysis. User-Friendly Interface: Ensures an intuitive and easy-to-navigate user experience. Security: Implements robust security measures to protect user data. Technologies Used: Backend: Flask: Utilized Flask to build a lightweight and scalable web application framework. MySQL: Employed MySQL for reliable and efficient database management. Frontend: HTML: Structured the content on the web pages. CSS: Styled the web pages for a visually appealing interface. JavaScript (JS): Added interactivity and enhanced user experience on the web pages."
    },
    {
        "repo_name": "Multilingual-ASR",
        "repo_link": "https://github.com/edwin16804/Multilingual-ASR",
        "readme": "Multilingual-ASR Datasets Common Voice dataset Multi-language, open-source voice dataset for training speech-enabled applications. Provides audio chunks and corresponding actual text transcriptions. Example code to load Hindi data: python common_voice_train = load_dataset( \"mozilla-foundation/common_voice_16_0\", \"hi\", split=\"train+validation\", trust_remote_code=True ) Models Wav2Vec2-BERT 580M-parameter versatile audio model, pre-trained on 4.5M hours of unlabeled audio data covering more than 143 languages. Predictions are done in a single pass, making it much faster than sequence-to-sequence models like Whisper. Achieves strong results even with small amounts of training data, adapts easily to any alphabet, and is resource-efficient. Expects input audio as a 1D array sampled at 16 kHz. Wav2Vec2 XLS-R XLS-R is Facebook AI's cross-lingual large-scale version of Wav2Vec2, trained on more than 436k hours of speech from 128 languages, allowing robust performance on multilingual ASR tasks. XLS-R models (300M, 1B parameters) excel for low-resource languages and offer improved accuracy and generalization compared to standard Wav2Vec2. The architecture includes a feature encoder, transformer layers, and quantization modules, refined for cross-lingual robustness and high performance with limited labeled data. XLS-R 300M is ideal for Hindi and other Indian languages due to its multilingual pretraining and demonstrated effectiveness in ASR settings. My Training Used the Wav2Vec2 Large XLS-R 300M architecture for fine-tuning Hindi ASR. Audio Format: Mono, 16 kHz WAV files. Dataset: Mozilla Common Voice Hindi (train + validation splits). Hyperparameters: Batch Size: 4 Gradient Accumulation: 8 Learning Rate: 3e-5 (Adam optimizer) Warmup Ratio: 0.1 Epochs: 30 Mixed Precision (FP16): Enabled (auto-switch with CUDA) Evaluation: Every 500 steps Performance: Achieved competitive Word Error Rate (WER) compared to baselines; model adapts well to Hindi alphabets. The training procedure was resource-efficient and achieved fast convergence thanks to the XLS-R\u2019s robust pretrained representations. Key Advantages Multilingual support: XLS-R can be adapted to any alphabet or language present in Common Voice. Speed and efficiency: Faster and more data-efficient than sequence-to-sequence ASR models. Superior results for Hindi: Outperformed Whisper and other baselines on Hindi Common Voice."
    },
    {
        "repo_name": "resume-job-similarity-app-AWS",
        "repo_link": "https://github.com/edwin16804/resume-job-similarity-app-AWS",
        "readme": "Job-Resume-Matcher A web-based application designed to match job descriptions with resumes by computing similarity scores. The project utilizes natural language processing (NLP) models for text summarization and semantic similarity analysis, providing a tool to assess the compatibility of resumes with job listings. Features Resume Text Extraction : Extract text from PDF resumes. Resume Summarization : Summarizes lengthy resume content to focus on key skills and experiences. Job Description Matching : Matches resumes with relevant job descriptions based on semantic similarity. Similarity Scoring : Outputs a similarity score (in percentage) between the resume and job descriptions. User-Friendly Interface : Allows easy uploading of PDF resumes for quick analysis. Tech Stack Frontend : React Axios (for making HTTP requests) Backend : Flask (Python web framework) Hugging Face Transformers for text summarization (BART model) Sentence-Transformers for computing semantic similarity (using pre-trained models) PDFMiner for extracting text from resumes Scikit-learn (for cosine similarity computation) Setup and Installation Prerequisites Before running the project, make sure you have the following installed: Python 3.x Node.js (for frontend) Backend Setup (Flask) Clone the repository: bash git clone https://github.com/your-username/job-resume-matcher.git cd job-resume-matcher Create a Python virtual environment: bash python -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate Install the required Python packages: bash pip install -r requirements.txt Download the pre-trained models (BART, Sentence Transformers) as specified in the app.py or modify the paths accordingly. Run the Flask backend server: bash python app.py The server will be running at http://localhost:5000 . Frontend Setup (React) Navigate to the frontend directory: bash cd frontend Install the required Node packages: bash npm install # or yarn install Run the React development server: bash npm start # or yarn start The app will be running at http://localhost:3000 . Working https://github.com/user-attachments/assets/7164f2d8-1bc6-4229-9b52-9c78529bcfc5 How It Works Upload a Resume : The user uploads a PDF resume. Text Extraction : The system extracts text from the resume using the PDFMiner library. Text Summarization : The extracted text is summarized using a fine-tuned BART model. Similarity Calculation : The summarized resume is compared with a list of predefined job descriptions using semantic similarity models (Sentence-Transformers). Display Results : The app displays similarity scores for each job description in percentage format. API Endpoints /ping Method : GET Description : Health check endpoint to verify that the backend is running. /upload/ Method : POST Description : Upload a PDF file containing the resume. Request : Form data: file (PDF file) Response : JSON with the following fields: status : success or error message filename : name of the uploaded file extracted_text : cleaned text from the resume summary : summarized resume content similarity : List of similarity scores with predefined job descriptions Notes: Model Files : Make sure to add instructions about downloading or providing paths for the BART and Sentence-Transformer models used in the backend. Customizations : Modify the app's behavior based on your own deployment preferences or API usage if needed."
    },
    {
        "repo_name": "Simple-RAG-application",
        "repo_link": "https://github.com/edwin16804/Simple-RAG-application",
        "readme": "Simple RAG System A simple Retrieval-Augmented Generation (RAG) system built with FastAPI that allows you to upload PDF documents and query them using natural language. The system uses vector embeddings for semantic search and provides contextual answers based on your documents. Features PDF Document Upload : Upload PDF files and automatically extract text content Vector Embeddings : Generate embeddings using Ollama's nomic-embed-text model Vector Database : Store and query embeddings using ChromaDB Semantic Search : Find relevant document sections based on your queries AI-Powered Responses : Get contextual answers using OpenRouter's GPT models RESTful API : Clean FastAPI endpoints for easy integration Architecture PDF Upload \u2192 Text Extraction \u2192 Embedding Generation \u2192 Vector Storage (ChromaDB) \u2193 User Query \u2192 Embedding Generation \u2192 Vector Search \u2192 Context Retrieval \u2192 AI Response Prerequisites Python 3.8+ Ollama installed and running locally ChromaDB account and credentials OpenRouter API key Installation Clone the repository bash git clone <repository-url> cd Simple-RAG-System Install dependencies bash pip install fastapi uvicorn langchain-community openai python-dotenv chromadb ollama Install and setup Ollama bash # Install Ollama (visit https://ollama.ai for installation instructions) ollama pull nomic-embed-text Environment Setup Create a .env file in the project root with the following variables: env OPENROUTER_API_KEY=your_openrouter_api_key CHROMA_API_KEY=your_chromadb_api_key CHROMA_TENANT_ID=your_chromadb_tenant_id CHROMA_DATABASE=your_chromadb_database_name Usage Start the server bash uvicorn main:app --reload Upload a PDF document bash curl -X POST \"http://localhost:8000/uploadfile/\" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@your_document.pdf\" Query your documents bash curl -X POST \"http://localhost:8000/chat/\" \\ -H \"Content-Type: application/json\" \\ -d '{\"text\": \"What is this document about?\"}' API Endpoints GET / Description : Health check endpoint Response : Simple status message POST /uploadfile/ Description : Upload and process a PDF file Parameters : file : PDF file (multipart/form-data) Response : json { \"filename\": \"document.pdf\", \"page_content\": \"First page content...\", \"embedding\": [0.1, 0.2, ...] } POST /chat/ Description : Query your uploaded documents Parameters : text : Your question (string) Response : json { \"query\": \"Your question\", \"answer\": \"AI-generated answer based on your documents\", \"similar_documents\": [ { \"page_content\": \"Relevant content...\", \"page_number\": 1, \"filename\": \"document.pdf\" } ] } Project Structure Simple-RAG-System/ \u251c\u2500\u2500 main.py # FastAPI application and endpoints \u251c\u2500\u2500 utils/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 chromadb_operations.py # ChromaDB vector operations \u2502 \u2514\u2500\u2500 generate_embeddings.py # Embedding generation using Ollama \u251c\u2500\u2500 README.md \u2514\u2500\u2500 .env # Environment variables (create this) Configuration ChromaDB Setup Create a ChromaDB Cloud account Create a database in ChromaDB Cloud The collection my_collection will be automatically created when you first run the application The collection uses 768-dimensional embeddings (for nomic-embed-text) Model Configuration Embedding Model : nomic-embed-text (via Ollama) LLM Model : openai/gpt-oss-20b:free (via OpenRouter) Dependencies FastAPI : Web framework LangChain : Document processing Ollama : Local embedding generation ChromaDB : Vector database OpenRouter : LLM API access OpenAI : API client Contributing Fork the repository Create a feature branch Make your changes Submit a pull request License This project is open source and available under the MIT License . Troubleshooting Common Issues Ollama not running : Make sure Ollama is installed and the nomic-embed-text model is pulled ChromaDB connection failed : Verify your API key, tenant ID, and database name in the .env file OpenRouter API errors : Check your API key and account status"
    },
    {
        "repo_name": "Smart-Energy-Management-System-Using-Ensemble-Learning",
        "repo_link": "https://github.com/edwin16804/Smart-Energy-Management-System-Using-Ensemble-Learning",
        "readme": "Smart Energy Management System Using Ensemble Learning Project Overview This project introduces a Smart Energy Management System (SEMS) that utilizes ensemble learning techniques, specifically AdaBoost and Random Forest classifiers, to predict and reduce energy wastage in smart homes. By processing environmental data such as brightness, humidity, and temperature, the system identifies unnecessary lighting use and optimizes energy consumption. The goal is to enhance energy efficiency in smart homes by implementing the most accurate predictive models. Objectives Develop a Smart Energy Management System (SEMS) to minimize energy wastage in smart homes. Utilize ensemble learning classifiers (AdaBoost and Random Forest) to predict energy waste with high accuracy. Compare the performance of these classifiers and select the best-performing model for deployment. System Architecture Data Collection Module : Utilizes Google Colab and Google Drive for computational resources and data access. Collects raw sensory data from smart home sensors (brightness, humidity, temperature). Data Preprocessing Subsystem : Cleans and structures data, handles missing values, and performs feature extraction and engineering. Clustering Mechanism : Applies K-Means algorithm to segment the data into clusters, enhancing the feature set for the ensemble models. Ensemble Learning Engine : Integrates AdaBoost and Random Forest classifiers to predict energy wastage scenarios effectively. Performance Evaluation and Model Selection : Assesses model accuracy using metrics like accuracy score, precision, recall, and F1-score. Selects the best-performing model for real-time decision-making. Decision-Making Module : Implements the selected model to predict energy wastage and control smart home devices. Feedback Loop : Continuously improves the system by retraining models with new data, ensuring adaptability and accuracy over time. Dataset Title : Open Smart Home Data Set Link : Dataset Link Description : The dataset includes time series data of brightness, humidity, and temperature from various rooms in a smart home, collected between March and June 2017. Results Random Forest Classifier : Accuracy : 99.97% Significant Features : Brightness (89.69% importance), followed by Humidity and Temperature. AdaBoost Classifier : Accuracy : 100.00% Combined Classifier : Strategy : Majority voting combining both classifiers. Accuracy : 100.00% Future Scope Real-time Data Processing : Enhance responsiveness by integrating real-time data processing. Expanded Dataset : Include more diverse environmental variables for improved accuracy. Advanced Models : Explore deep learning approaches for more nuanced energy usage insights. IoT Integration : Collaborate with IoT devices for automated control based on system feedback. How to Run Download the dataset and Jupyter notebook from the drive link . Ensure all files are in the same location. Run the program in Jupyter Notebook. License The dataset is subject to copyright (c) 2018 by the Fraunhofer Institute for Building Physics, N\u00fcrnberg, Germany. Conclusion The Smart Energy Management System, leveraging AdaBoost and Random Forest classifiers, showcases an innovative approach to reducing energy wastage in smart homes. By integrating environmental data and employing ensemble learning techniques, this system offers a reliable and effective solution for smart home energy management. Authors Rohan Menon - B.Tech. CSE(AI&ML) Edwin Chazhoor - B.Tech. CSE(AI&ML) This README file is designed to provide a comprehensive overview of the project, guiding potential users or collaborators on how to understand, run, and extend the project."
    },
    {
        "repo_name": "StepKicks-AWS-Sneaker-Store",
        "repo_link": "https://github.com/edwin16804/StepKicks-AWS-Sneaker-Store",
        "readme": "Sneaker Store Web App Overview Welcome to the Sneaker Store Web App! This is a full-stack application designed for sneaker enthusiasts, enabling users to browse, place orders, and manage their sneaker purchases effortlessly. The app leverages modern web technologies and AWS services to ensure a smooth, fast, and scalable experience. Features \ud83d\uded2 Place Orders: Users can select sneakers and place their orders seamlessly. \ud83d\udccb View Orders: View a list of all orders placed, complete with details like sneaker type, quantity, and price. \u270f\ufe0f Modify Orders: Update the details of existing orders, such as changing quantities or sneaker type. \u274c Delete Orders: Cancel orders with ease. Tech Stack Frontend React.js: Used to build a responsive and interactive user interface. Axios: Handles communication with the backend via HTTP requests. Backend AWS Lambda: Serverless backend to handle CRUD operations for orders. DynamoDB: NoSQL database to store order details. API Integration The frontend communicates with the backend through AWS API Gateway, ensuring a secure and efficient flow of data between components."
    },
    {
        "repo_name": "Surface-Crack-Detection-CNN-MV-Techniques",
        "repo_link": "https://github.com/edwin16804/Surface-Crack-Detection-CNN-MV-Techniques",
        "readme": "Surface-Crack-Detection-CNN-MV-Techniques"
    }
]