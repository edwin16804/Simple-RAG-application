[
    {
        "repo_name": "Backend-Projects",
        "repo_link": "https://github.com/edwin16804/Backend-Projects",
        "readme": "Backend-Projects\n\n\n\n\nhttps://roadmap.sh/projects/github-user-activity\n\n\nhttps://roadmap.sh/projects/expense-tracker\n\n\nhttps://roadmap.sh/projects/weather-api-wrapper-service"
    },
    {
        "repo_name": "Bajaj_Finserv_FullStack_RestAPI",
        "repo_link": "https://github.com/edwin16804/Bajaj_Finserv_FullStack_RestAPI",
        "readme": "Bajaj Finserv FullStack REST API\n\n\nA FastAPI-based REST API that processes input data arrays and categorizes elements while performing various transformations.\n\n\nFeatures\n\n\n\n\nData Processing\n: Categorizes input elements into odd numbers, even numbers, alphabets, and special characters\n\n\nMathematical Operations\n: Calculates sum of all numeric elements\n\n\nString Transformations\n: Applies alternating case transformation to concatenated alphabetic strings\n\n\nRESTful API\n: Clean POST endpoint for data processing\n\n\n\n\nAPI Endpoint\n\n\nPOST \n/bfhl\n\n\nProcesses an array of data and returns categorized results.\n\n\nRequest Body:\n\n\njson\n{\n    \"data\": [\"string1\", \"string2\", \"123\", \"456\", \"abc\", \"def\", \"!@#\"]\n}\n\n\nResponse:\n\n\njson\n{\n    \"is_success\": true,\n    \"user_id\": \"john_doe_17091999\",\n    \"email\": \"john@xyz.com\",\n    \"roll_number\": \"ABCD123\",\n    \"odd_numbers\": [\"123\"],\n    \"even_numbers\": [\"456\"],\n    \"alphabets\": [\"ABC\", \"DEF\"],\n    \"special_characters\": [\"!@#\"],\n    \"sum\": \"579\",\n    \"concat_string\": \"cBa\"\n}\n\n\nInstallation\n\n\n\n\n\n\nClone the repository:\n\n\nbash\n   git clone <your-repo-url>\n   cd Bajaj_Finserv_FullStack_RestAPI\n\n\n\n\n\n\nInstall dependencies:\n\n\nbash\n   pip install -r requirements.txt\n\n\n\n\n\n\nRunning the Application\n\n\nDevelopment Server\n\n\n\n\n\n\nStart the FastAPI server:\n\n\nbash\n   uvicorn main:app --reload\n\n\n\n\n\n\nAccess the application:\n\n\n\n\nAPI: http://localhost:8000\n\n\nInteractive API docs: http://localhost:8000/docs\n\n\nReDoc documentation: http://localhost:8000/redoc\n\n\n\n\nProduction Server\n\n\nbash\nuvicorn main:app --host 0.0.0.0 --port 8000\n\n\nAPI Testing\n\n\nUsing curl\n\n\nbash\ncurl -X POST \"http://localhost:8000/bfhl\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"data\": [\"123\", \"456\", \"abc\", \"def\", \"!@#\"]}'\n\n\nUsing Python requests\n\n\n```python\nimport requests\n\n\nurl = \"http://localhost:8000/bfhl\"\ndata = {\"data\": [\"123\", \"456\", \"abc\", \"def\", \"!@#\"]}\nresponse = requests.post(url, json=data)\nprint(response.json())\n```\n\n\nDependencies\n\n\n\n\nFastAPI\n: Modern, fast web framework for building APIs\n\n\nPydantic\n: Data validation using Python type annotations\n\n\nUvicorn\n: ASGI server for running FastAPI applications\n\n\ntyping-extensions\n: Extended typing support\n\n\n\n\nProject Structure\n\n\nBajaj_Finserv_FullStack_RestAPI/\n\u251c\u2500\u2500 main.py              # FastAPI application and API endpoints\n\u251c\u2500\u2500 requirements.txt     # Python dependencies\n\u2514\u2500\u2500 README.md           # Project documentation\n\n\nData Processing Logic\n\n\n\n\nNumeric Elements\n: Separated into odd and even numbers, summed together\n\n\nAlphabetic Elements\n: Converted to uppercase and concatenated\n\n\nSpecial Characters\n: Identified and collected separately\n\n\nString Transformation\n: Concatenated alphabetic string is reversed and converted to alternating case"
    },
    {
        "repo_name": "Bird_Classification",
        "repo_link": "https://github.com/edwin16804/Bird_Classification",
        "readme": "Bird_Classification"
    },
    {
        "repo_name": "Curvetopia--Shape-Regularization-Symmetry-Detection-and-Image-Contour-Processing",
        "repo_link": "https://github.com/edwin16804/Curvetopia--Shape-Regularization-Symmetry-Detection-and-Image-Contour-Processing",
        "readme": "Curvetopia--Shape-Regularization-Symmetry-Detection-and-Image-Contour-Processing\n\n\nProject Description: Shape Regularization, Symmetry Detection, and Contour-Based Image Processing\n\n\nThis project involves developing a comprehensive Python-based tool for processing geometric shapes, both from CSV files and images. The tool reads path data from CSV files or processes shapes directly from images, visualizes the paths, regularizes the shapes to conform to ideal geometric forms (such as straight lines, circles, and rectangles), and then detects and reports symmetry within these shapes. Additionally, it includes functionality for contour detection and shape filling in images. The output includes visual plots of the original and regularized shapes, files saved in SVG and PNG formats, and processed images with filled contours.\n\n\nOverview\n\n\nShapes represented as sequences of points or within images may not always perfectly align with ideal geometric forms due to noise, imperfections in data collection, or other factors. This project aims to regularize these shapes, meaning it transforms them into their ideal forms while maintaining their general structure. The tool then analyzes these regularized shapes to detect any inherent symmetries, such as symmetry along the x-axis, y-axis, or diagonal lines. For image data, the project includes an additional step of detecting and filling contours, which can be useful for visualizing and analyzing the structural components of the shapes.\n\n\nFunctionality\n\n\n\n\n\n\nCSV File Reading\n: The tool reads a CSV file where each row represents a point in a path, with paths grouped by a unique identifier in the first column. The \nread_csv\n function organizes these points into paths based on the unique identifiers, making it easier to process them as distinct shapes.\n\n\n\n\n\n\nImage Processing with OpenCV\n: For images, the tool includes a process that uses OpenCV to detect and fill shapes:\n\n\n\n\nLoading and Grayscaling\n: The image is loaded from a PNG file and converted to grayscale to simplify the analysis.\n\n\nEdge Detection\n: Gaussian blur is applied to reduce noise, followed by Canny edge detection to identify the edges within the image.\n\n\nContour Detection and Filling\n: The contours of the shapes are detected from the edges, and these contours are filled with a color (e.g., green) to highlight the shapes in the image. The processed image is then saved and displayed.\n\n\n\n\n\n\n\n\nVisualization\n: The \nplot\n function provides a visual representation of the shapes as they are initially read from the CSV, while \ncv2_imshow\n displays the original and processed images, allowing for a direct comparison of the before and after states.\n\n\n\n\n\n\nShape Regularization\n: The \nregularize_shape\n function applies geometric rules to convert irregular point sequences into idealized shapes.\n\n\n\n\nStraight Line Detection\n: If a shape is identified as nearly straight, it is reduced to a line segment between the first and last points.\n\n\nCircle Detection\n: For shapes approximating a circle, the tool calculates the average radius and repositions the points evenly around the center.\n\n\nRectangle Detection\n: Shapes resembling rectangles are adjusted to form a perfect rectangle with aligned edges.\n\n\n\n\n\n\n\n\nSymmetry Detection\n: The tool checks for symmetry in regularized shapes using the \nfind_symmetry\n function. It assesses symmetry across various axes and diagonal lines and outputs whether the shape is symmetric about the x-axis, y-axis, or along lines such as y=x or y=-x.\n\n\n\n\n\n\nSVG and PNG Output\n: The \npolylines2svg\n function converts the regularized shapes into SVG format, a vector graphics format that allows for scalable and detailed visualization. Additionally, the tool saves the output as a PNG image, ensuring compatibility with a wide range of applications.\n\n\n\n\n\n\nComplete Workflow\n: The \nprocess_file\n function integrates all these steps, from reading the CSV to producing the final outputs. It also prints the detected symmetries to the console, providing immediate feedback on the analysis.\n\n\n\n\n\n\nApplications\n\n\nThis tool is versatile and can be used in various fields requiring geometric analysis, such as computer graphics, architectural design, and pattern recognition. The inclusion of image processing capabilities extends its utility to scenarios where shapes are embedded within images, allowing for contour detection and filling, which are essential in image segmentation tasks.\n\n\nConclusion\n\n\nThis project offers a robust solution for reading, regularizing, visualizing, analyzing geometric shapes from CSV files, and processing shapes directly from images. Through its multifaceted functionality, it ensures that even imperfectly defined shapes can be regularized, analyzed for symmetry, and visualized in both vector and raster formats, providing clear and accurate geometric insights."
    },
    {
        "repo_name": "DL-based-IoT-intrusion-detection",
        "repo_link": "https://github.com/edwin16804/DL-based-IoT-intrusion-detection",
        "readme": "IoT Intrusion Detection Using Machine Learning & Deep Learning\n\n\nProject Overview\n\n\nThis project focuses on \nintrusion detection in IoT devices\n using \nmachine learning and deep learning models\n. With the increasing number of IoT devices, cybersecurity threats are becoming more sophisticated. This study aims to build an efficient \nIntrusion Detection System (IDS)\n that can classify various types of network attacks using the \nCICIOT2023 dataset\n.  \n\n\nModels Used\n\n\nWe implemented and compared the following models:  \n\n\nDeep Learning Models\n\n\n\n\nLSTM (Long Short-Term Memory)\n \u2013 Best for sequential data (Achieved \n99.89% accuracy\n).  \n\n\nGRU (Gated Recurrent Unit)\n \u2013 Similar to LSTM but computationally more efficient (\n98.69% accuracy\n).  \n\n\nDeep CNN (Convolutional Neural Network)\n \u2013 Extracts spatial patterns from network traffic (\n98.34% accuracy\n).  \n\n\n\n\nMachine Learning Models\n\n\n\n\nDecision Tree\n \u2013 Simple yet effective rule-based classification (\n99.37% accuracy\n).  \n\n\nXGBoost (Extreme Gradient Boosting)\n \u2013 Fast and accurate boosting algorithm (\n99.30% accuracy\n).  \n\n\n\n\nDataset\n\n\nWe used the \nCICIOT2023 dataset\n, which consists of:\n\n- \n105 IoT devices\n (Smart home, Zigbee, Z-Wave, etc.)\n\n- \n33 different attack types\n (DDoS, DoS, Brute Force, Mirai, etc.)\n\n- \n47 features\n extracted from network traffic logs.  \n\n\nPreprocessing Steps\n\n\n\n\nHandling Missing Values\n \n\n\nFeature Selection Using Correlation Analysis\n \n\n\nLabel Encoding for Categorical Features\n \n\n\nStandardization Using StandardScaler\n \n\n\nSplitting Dataset (70% Train, 30% Test)\n \n\n\n\n\nModel Training & Evaluation\n\n\nEach model was trained and evaluated using the following metrics:\n\n\nAccuracy\n\n\nPrecision, Recall, F1-score\n\n\nFalse Positive Rate (FPR)\n \n\n\n| Model       | Accuracy  | Precision | Recall | F1-score |\n|------------|----------|-----------|--------|----------|\n| \nLSTM\n        | \n99.89%\n   | 99.72%    | 99.85% | 99.78%   |\n| \nGRU\n         | 98.69%   | 98.41%    | 98.54% | 98.47%   | \n| \nDeep CNN\n    | 98.34%   | 98.58%    | 97.00% | 97.20%   | \n| \nDecision Tree\n | 99.37%  | 99.00%    | 99.10% | 99.05%   | \n| \nXGBoost\n     | 99.30%   | 0.85 (macro avg) | 0.73 (macro avg) | 0.75 (macro avg) | \n\n\nKey Findings\n\n\nLSTM performed best\n, achieving \n99.89% accuracy\n, but required longer training time.\n\n\nGRU and CNN models also showed strong performance\n with high accuracy and stability.\n\n\nDecision Trees & XGBoost were faster\n and still highly accurate, making them suitable for real-time detection.\n\n\nClass imbalance affected recall for rare attack types\n (e.g., Backdoor Malware, Uploading Attack)."
    },
    {
        "repo_name": "Engineers_Salary_Prediction",
        "repo_link": "https://github.com/edwin16804/Engineers_Salary_Prediction",
        "readme": "Engineers_Salary_Prediction"
    },
    {
        "repo_name": "Finance-Tracker",
        "repo_link": "https://github.com/edwin16804/Finance-Tracker",
        "readme": "Personal Finance Tracker\n\n\nDeveloped a Personal Finance Manager to help users effectively manage their finances by organizing and optimizing income, expenses, savings, and investments.\n\n\nKey Features:\nIncome Tracking: Allows users to record and monitor all sources of income.\nExpense Management: Enables users to categorize and track all expenses.\nSavings Optimization: Provides tools and insights to help users save more effectively.\nInvestment Tracking: Monitors investments and provides performance analysis.\nUser-Friendly Interface: Ensures an intuitive and easy-to-navigate user experience.\nSecurity: Implements robust security measures to protect user data.\n\n\nTechnologies Used:\nBackend:\nFlask: Utilized Flask to build a lightweight and scalable web application framework.\nMySQL: Employed MySQL for reliable and efficient database management.\nFrontend:\nHTML: Structured the content on the web pages.\nCSS: Styled the web pages for a visually appealing interface.\nJavaScript (JS): Added interactivity and enhanced user experience on the web pages."
    },
    {
        "repo_name": "FullStack-React-App-AWS",
        "repo_link": "https://github.com/edwin16804/FullStack-React-App-AWS",
        "readme": "Sneaker Store Web App\n\n\nOverview\n\n\nWelcome to the Sneaker Store Web App! This is a full-stack application designed for sneaker enthusiasts, enabling users to browse, place orders, and manage their sneaker purchases effortlessly. The app leverages modern web technologies and AWS services to ensure a smooth, fast, and scalable experience.\n\n\nFeatures\n\n\n\ud83d\uded2 Place Orders: Users can select sneakers and place their orders seamlessly.\n\ud83d\udccb View Orders: View a list of all orders placed, complete with details like sneaker type, quantity, and price.\n\u270f\ufe0f Modify Orders: Update the details of existing orders, such as changing quantities or sneaker type.\n\u274c Delete Orders: Cancel orders with ease.\n\n\nTech Stack\n\n\nFrontend\n\n\nReact.js: Used to build a responsive and interactive user interface.\nAxios: Handles communication with the backend via HTTP requests.\n\n\nBackend\n\n\nAWS Lambda: Serverless backend to handle CRUD operations for orders.\nDynamoDB: NoSQL database to store order details.\nAPI Integration\nThe frontend communicates with the backend through AWS API Gateway, ensuring a secure and efficient flow of data between components."
    },
    {
        "repo_name": "Job-listing",
        "repo_link": "https://github.com/edwin16804/Job-listing",
        "readme": "Internet Web Programming \nProject details:\nProject: Job Listing Website\nProject Members: \nAnand Sreekumar\nSai Pranav Thota\nJaideep Naidu\nAkshat Raghunath"
    },
    {
        "repo_name": "Job-recommendation",
        "repo_link": "https://github.com/edwin16804/Job-recommendation",
        "readme": "Job-recommendation"
    },
    {
        "repo_name": "Multilingual-ASR",
        "repo_link": "https://github.com/edwin16804/Multilingual-ASR",
        "readme": "Multilingual-ASR\n\n\nDatasets\n\n\n\n\nCommon Voice dataset\n\n\nMulti-language, open-source voice dataset for training speech-enabled applications.\n\n\nProvides audio chunks and corresponding actual text transcriptions.\n\n\nExample code to load Hindi data:\n  \npython\n  common_voice_train = load_dataset(\n      \"mozilla-foundation/common_voice_16_0\",\n      \"hi\",\n      split=\"train+validation\",\n      trust_remote_code=True\n  )\n\n\n\n\n\n\n\n\nModels\n\n\n\n\n\n\nWav2Vec2-BERT\n\n\n\n\n580M-parameter versatile audio model, pre-trained on 4.5M hours of unlabeled audio data covering more than 143 languages.\n\n\nPredictions are done in a single pass, making it much faster than sequence-to-sequence models like Whisper.\n\n\nAchieves strong results even with small amounts of training data, adapts easily to any alphabet, and is resource-efficient.\n\n\nExpects input audio as a 1D array sampled at 16 kHz.\n\n\n\n\n\n\n\n\nWav2Vec2 XLS-R\n\n\n\n\nXLS-R is Facebook AI's cross-lingual large-scale version of Wav2Vec2, trained on more than 436k hours of speech from 128 languages, allowing robust performance on multilingual ASR tasks.\n\n\nXLS-R models (300M, 1B parameters) excel for low-resource languages and offer improved accuracy and generalization compared to standard Wav2Vec2.\n\n\nThe architecture includes a feature encoder, transformer layers, and quantization modules, refined for cross-lingual robustness and high performance with limited labeled data.\n\n\nXLS-R 300M is ideal for Hindi and other Indian languages due to its multilingual pretraining and demonstrated effectiveness in ASR settings.\n\n\n\n\n\n\n\n\nMy Training\n\n\n\n\nUsed the \nWav2Vec2 Large XLS-R 300M\n architecture for fine-tuning Hindi ASR.\n\n\nAudio Format:\n Mono, 16 kHz WAV files.\n\n\nDataset:\n Mozilla Common Voice Hindi (train + validation splits).\n\n\nHyperparameters:\n\n\nBatch Size: 4\n\n\nGradient Accumulation: 8\n\n\nLearning Rate: 3e-5 (Adam optimizer)\n\n\nWarmup Ratio: 0.1\n\n\nEpochs: 30\n\n\nMixed Precision (FP16): Enabled (auto-switch with CUDA)\n\n\nEvaluation: Every 500 steps\n\n\n\n\n\n\nPerformance:\n Achieved competitive Word Error Rate (WER) compared to baselines; model adapts well to Hindi alphabets.\n\n\nThe training procedure was resource-efficient and achieved fast convergence thanks to the XLS-R\u2019s robust pretrained representations.\n\n\n\n\nKey Advantages\n\n\n\n\nMultilingual support:\n XLS-R can be adapted to any alphabet or language present in Common Voice.\n\n\nSpeed and efficiency:\n Faster and more data-efficient than sequence-to-sequence ASR models.\n\n\nSuperior results for Hindi:\n Outperformed Whisper and other baselines on Hindi Common Voice."
    },
    {
        "repo_name": "nocturnal-hotel-management-system",
        "repo_link": "https://github.com/edwin16804/nocturnal-hotel-management-system",
        "readme": "Hotel Management System\n\n\nObjective:\nDeveloped a comprehensive Hotel Management System providing users with access to all essential features required for efficient hotel management, such as room booking, check-out, and reservations.\n\n\nKey Features:\nRoom Booking: Allows users to book rooms based on availability and preferences.\nCheck-Out Process: Manages the check-out process, including billing and payment handling.\nReservations: Facilitates making, modifying, and canceling room reservations.\n\n\nTechnologies Used:\nProgramming Language:\nPython: Utilized Python for the core application logic.\nTkinter Module: Used Tkinter for building the graphical user interface (GUI) of the application.\nDatabase Management:\nMySQL: Employed MySQL for managing and storing hotel data efficiently"
    },
    {
        "repo_name": "resume-job-similarity-app-AWS",
        "repo_link": "https://github.com/edwin16804/resume-job-similarity-app-AWS",
        "readme": "Job-Resume-Matcher\n\n\nA web-based application designed to match job descriptions with resumes by computing similarity scores. The project utilizes natural language processing (NLP) models for text summarization and semantic similarity analysis, providing a tool to assess the compatibility of resumes with job listings.\n\n\nFeatures\n\n\n\n\nResume Text Extraction\n: Extract text from PDF resumes.\n\n\nResume Summarization\n: Summarizes lengthy resume content to focus on key skills and experiences.\n\n\nJob Description Matching\n: Matches resumes with relevant job descriptions based on semantic similarity.\n\n\nSimilarity Scoring\n: Outputs a similarity score (in percentage) between the resume and job descriptions.\n\n\nUser-Friendly Interface\n: Allows easy uploading of PDF resumes for quick analysis.\n\n\n\n\nTech Stack\n\n\n\n\nFrontend\n:\n\n\nReact\n\n\n\n\nAxios (for making HTTP requests)\n\n\n\n\n\n\nBackend\n:\n\n\n\n\nFlask (Python web framework)\n\n\nHugging Face Transformers for text summarization (BART model)\n\n\nSentence-Transformers for computing semantic similarity (using pre-trained models)\n\n\nPDFMiner for extracting text from resumes\n\n\nScikit-learn (for cosine similarity computation)\n\n\n\n\nSetup and Installation\n\n\nPrerequisites\n\n\nBefore running the project, make sure you have the following installed:\n\n\n\n\nPython 3.x\n\n\nNode.js (for frontend)\n\n\n\n\nBackend Setup (Flask)\n\n\n\n\n\n\nClone the repository:\n   \nbash\n   git clone https://github.com/your-username/job-resume-matcher.git\n   cd job-resume-matcher\n\n\n\n\n\n\nCreate a Python virtual environment:\n   \nbash\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n\n\n\n\n\nInstall the required Python packages:\n   \nbash\n   pip install -r requirements.txt\n\n\n\n\n\n\nDownload the pre-trained models (BART, Sentence Transformers) as specified in the \napp.py\n or modify the paths accordingly.\n\n\n\n\n\n\nRun the Flask backend server:\n   \nbash\n   python app.py\n\n   The server will be running at \nhttp://localhost:5000\n.\n\n\n\n\n\n\nFrontend Setup (React)\n\n\n\n\n\n\nNavigate to the frontend directory:\n   \nbash\n   cd frontend\n\n\n\n\n\n\nInstall the required Node packages:\n   \nbash\n   npm install  # or yarn install\n\n\n\n\n\n\nRun the React development server:\n   \nbash\n   npm start  # or yarn start\n\n   The app will be running at \nhttp://localhost:3000\n.\n\n\n\n\n\n\nWorking\n\n\nhttps://github.com/user-attachments/assets/7164f2d8-1bc6-4229-9b52-9c78529bcfc5\n\n\nHow It Works\n\n\n\n\nUpload a Resume\n: The user uploads a PDF resume.\n\n\nText Extraction\n: The system extracts text from the resume using the \nPDFMiner\n library.\n\n\nText Summarization\n: The extracted text is summarized using a fine-tuned BART model.\n\n\nSimilarity Calculation\n: The summarized resume is compared with a list of predefined job descriptions using semantic similarity models (Sentence-Transformers).\n\n\nDisplay Results\n: The app displays similarity scores for each job description in percentage format.\n\n\n\n\nAPI Endpoints\n\n\n/ping\n\n\n\n\nMethod\n: GET\n\n\nDescription\n: Health check endpoint to verify that the backend is running.\n\n\n\n\n/upload/\n\n\n\n\nMethod\n: POST\n\n\nDescription\n: Upload a PDF file containing the resume.\n\n\nRequest\n: \n\n\nForm data: \nfile\n (PDF file)\n\n\nResponse\n: JSON with the following fields:\n\n\nstatus\n: success or error message\n\n\nfilename\n: name of the uploaded file\n\n\nextracted_text\n: cleaned text from the resume\n\n\nsummary\n: summarized resume content\n\n\nsimilarity\n: List of similarity scores with predefined job descriptions\n\n\n\n\n\n\nNotes:\n\n\n\n\nModel Files\n: Make sure to add instructions about downloading or providing paths for the BART and Sentence-Transformer models used in the backend.\n\n\nCustomizations\n: Modify the app's behavior based on your own deployment preferences or API usage if needed."
    },
    {
        "repo_name": "Simple-RAG-application",
        "repo_link": "https://github.com/edwin16804/Simple-RAG-application",
        "readme": "Simple RAG System\n\n\nA simple Retrieval-Augmented Generation (RAG) system built with FastAPI that allows you to upload PDF documents and query them using natural language. The system uses vector embeddings for semantic search and provides contextual answers based on your documents.\n\n\nFeatures\n\n\n\n\nPDF Document Upload\n: Upload PDF files and automatically extract text content\n\n\nVector Embeddings\n: Generate embeddings using Ollama's \nnomic-embed-text\n model\n\n\nVector Database\n: Store and query embeddings using ChromaDB\n\n\nSemantic Search\n: Find relevant document sections based on your queries\n\n\nAI-Powered Responses\n: Get contextual answers using OpenRouter's GPT models\n\n\nRESTful API\n: Clean FastAPI endpoints for easy integration\n\n\n\n\nArchitecture\n\n\nPDF Upload \u2192 Text Extraction \u2192 Embedding Generation \u2192 Vector Storage (ChromaDB)\n                                                           \u2193\nUser Query \u2192 Embedding Generation \u2192 Vector Search \u2192 Context Retrieval \u2192 AI Response\n\n\nPrerequisites\n\n\n\n\nPython 3.8+\n\n\nOllama installed and running locally\n\n\nChromaDB account and credentials\n\n\nOpenRouter API key\n\n\n\n\nInstallation\n\n\n\n\n\n\nClone the repository\n\n\nbash\n   git clone <repository-url>\n   cd Simple-RAG-System\n\n\n\n\n\n\nInstall dependencies\n\n\nbash\n   pip install fastapi uvicorn langchain-community openai python-dotenv chromadb ollama\n\n\n\n\n\n\nInstall and setup Ollama\n\n\nbash\n   # Install Ollama (visit https://ollama.ai for installation instructions)\n   ollama pull nomic-embed-text\n\n\n\n\n\n\nEnvironment Setup\n\n   Create a \n.env\n file in the project root with the following variables:\n   \nenv\n   OPENROUTER_API_KEY=your_openrouter_api_key\n   CHROMA_API_KEY=your_chromadb_api_key\n   CHROMA_TENANT_ID=your_chromadb_tenant_id\n   CHROMA_DATABASE=your_chromadb_database_name\n\n\n\n\n\n\nUsage\n\n\n\n\n\n\nStart the server\n\n\nbash\n   uvicorn main:app --reload\n\n\n\n\n\n\nUpload a PDF document\n\n\nbash\n   curl -X POST \"http://localhost:8000/uploadfile/\" \\\n        -H \"Content-Type: multipart/form-data\" \\\n        -F \"file=@your_document.pdf\"\n\n\n\n\n\n\nQuery your documents\n\n\nbash\n   curl -X POST \"http://localhost:8000/chat/\" \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"text\": \"What is this document about?\"}'\n\n\n\n\n\n\nAPI Endpoints\n\n\nGET /\n\n\n\n\nDescription\n: Health check endpoint\n\n\nResponse\n: Simple status message\n\n\n\n\nPOST /uploadfile/\n\n\n\n\nDescription\n: Upload and process a PDF file\n\n\nParameters\n: \n\n\nfile\n: PDF file (multipart/form-data)\n\n\nResponse\n: \n  \njson\n  {\n    \"filename\": \"document.pdf\",\n    \"page_content\": \"First page content...\",\n    \"embedding\": [0.1, 0.2, ...]\n  }\n\n\n\n\nPOST /chat/\n\n\n\n\nDescription\n: Query your uploaded documents\n\n\nParameters\n:\n\n\ntext\n: Your question (string)\n\n\nResponse\n:\n  \njson\n  {\n    \"query\": \"Your question\",\n    \"answer\": \"AI-generated answer based on your documents\",\n    \"similar_documents\": [\n      {\n        \"page_content\": \"Relevant content...\",\n        \"page_number\": 1,\n        \"filename\": \"document.pdf\"\n      }\n    ]\n  }\n\n\n\n\nProject Structure\n\n\nSimple-RAG-System/\n\u251c\u2500\u2500 main.py                    # FastAPI application and endpoints\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 chromadb_operations.py # ChromaDB vector operations\n\u2502   \u2514\u2500\u2500 generate_embeddings.py # Embedding generation using Ollama\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 .env                       # Environment variables (create this)\n\n\nConfiguration\n\n\nChromaDB Setup\n\n\n\n\nCreate a ChromaDB Cloud account\n\n\nCreate a database in ChromaDB Cloud\n\n\nThe collection \nmy_collection\n will be automatically created when you first run the application\n\n\nThe collection uses 768-dimensional embeddings (for nomic-embed-text)\n\n\n\n\nModel Configuration\n\n\n\n\nEmbedding Model\n: \nnomic-embed-text\n (via Ollama)\n\n\nLLM Model\n: \nopenai/gpt-oss-20b:free\n (via OpenRouter)\n\n\n\n\nDependencies\n\n\n\n\nFastAPI\n: Web framework\n\n\nLangChain\n: Document processing\n\n\nOllama\n: Local embedding generation\n\n\nChromaDB\n: Vector database\n\n\nOpenRouter\n: LLM API access\n\n\nOpenAI\n: API client\n\n\n\n\nContributing\n\n\n\n\nFork the repository\n\n\nCreate a feature branch\n\n\nMake your changes\n\n\nSubmit a pull request\n\n\n\n\nLicense\n\n\nThis project is open source and available under the \nMIT License\n.\n\n\nTroubleshooting\n\n\nCommon Issues\n\n\n\n\nOllama not running\n: Make sure Ollama is installed and the \nnomic-embed-text\n model is pulled\n\n\nChromaDB connection failed\n: Verify your API key, tenant ID, and database name in the \n.env\n file\n\n\nOpenRouter API errors\n: Check your API key and account status"
    },
    {
        "repo_name": "Smart-Energy-Management-System-Using-Ensemble-Learning",
        "repo_link": "https://github.com/edwin16804/Smart-Energy-Management-System-Using-Ensemble-Learning",
        "readme": "Smart Energy Management System Using Ensemble Learning\n\n\nProject Overview\n\n\nThis project introduces a Smart Energy Management System (SEMS) that utilizes ensemble learning techniques, specifically AdaBoost and Random Forest classifiers, to predict and reduce energy wastage in smart homes. By processing environmental data such as brightness, humidity, and temperature, the system identifies unnecessary lighting use and optimizes energy consumption. The goal is to enhance energy efficiency in smart homes by implementing the most accurate predictive models.\n\n\nObjectives\n\n\n\n\nDevelop a Smart Energy Management System (SEMS) to minimize energy wastage in smart homes.\n\n\nUtilize ensemble learning classifiers (AdaBoost and Random Forest) to predict energy waste with high accuracy.\n\n\nCompare the performance of these classifiers and select the best-performing model for deployment.\n\n\n\n\nSystem Architecture\n\n\n\n\nData Collection Module\n:\n\n\nUtilizes Google Colab and Google Drive for computational resources and data access.\n\n\n\n\nCollects raw sensory data from smart home sensors (brightness, humidity, temperature).\n\n\n\n\n\n\nData Preprocessing Subsystem\n:\n\n\n\n\n\n\nCleans and structures data, handles missing values, and performs feature extraction and engineering.\n\n\n\n\n\n\nClustering Mechanism\n:\n\n\n\n\n\n\nApplies K-Means algorithm to segment the data into clusters, enhancing the feature set for the ensemble models.\n\n\n\n\n\n\nEnsemble Learning Engine\n:\n\n\n\n\n\n\nIntegrates AdaBoost and Random Forest classifiers to predict energy wastage scenarios effectively.\n\n\n\n\n\n\nPerformance Evaluation and Model Selection\n:\n\n\n\n\nAssesses model accuracy using metrics like accuracy score, precision, recall, and F1-score.\n\n\n\n\nSelects the best-performing model for real-time decision-making.\n\n\n\n\n\n\nDecision-Making Module\n:\n\n\n\n\n\n\nImplements the selected model to predict energy wastage and control smart home devices.\n\n\n\n\n\n\nFeedback Loop\n:\n\n\n\n\nContinuously improves the system by retraining models with new data, ensuring adaptability and accuracy over time.\n\n\n\n\nDataset\n\n\n\n\nTitle\n: Open Smart Home Data Set\n\n\nLink\n: \nDataset Link\n\n\nDescription\n: The dataset includes time series data of brightness, humidity, and temperature from various rooms in a smart home, collected between March and June 2017.\n\n\n\n\nResults\n\n\n\n\nRandom Forest Classifier\n:\n\n\nAccuracy\n: 99.97%\n\n\n\n\nSignificant Features\n: Brightness (89.69% importance), followed by Humidity and Temperature.\n\n\n\n\n\n\nAdaBoost Classifier\n:\n\n\n\n\n\n\nAccuracy\n: 100.00%\n\n\n\n\n\n\nCombined Classifier\n:\n\n\n\n\nStrategy\n: Majority voting combining both classifiers.\n\n\nAccuracy\n: 100.00%\n\n\n\n\nFuture Scope\n\n\n\n\nReal-time Data Processing\n: Enhance responsiveness by integrating real-time data processing.\n\n\nExpanded Dataset\n: Include more diverse environmental variables for improved accuracy.\n\n\nAdvanced Models\n: Explore deep learning approaches for more nuanced energy usage insights.\n\n\nIoT Integration\n: Collaborate with IoT devices for automated control based on system feedback.\n\n\n\n\nHow to Run\n\n\n\n\nDownload the dataset and Jupyter notebook from the \ndrive link\n.\n\n\nEnsure all files are in the same location.\n\n\nRun the program in Jupyter Notebook.\n\n\n\n\nLicense\n\n\n\n\nThe dataset is subject to copyright (c) 2018 by the Fraunhofer Institute for Building Physics, N\u00fcrnberg, Germany.\n\n\n\n\nConclusion\n\n\nThe Smart Energy Management System, leveraging AdaBoost and Random Forest classifiers, showcases an innovative approach to reducing energy wastage in smart homes. By integrating environmental data and employing ensemble learning techniques, this system offers a reliable and effective solution for smart home energy management.\n\n\nAuthors\n\n\n\n\nRohan Menon\n - B.Tech. CSE(AI&ML)\n\n\nEdwin Chazhoor\n - B.Tech. CSE(AI&ML)\n\n\n\n\n\n\nThis README file is designed to provide a comprehensive overview of the project, guiding potential users or collaborators on how to understand, run, and extend the project."
    },
    {
        "repo_name": "sneaker-store",
        "repo_link": "https://github.com/edwin16804/sneaker-store",
        "readme": "Getting Started with Create React App\n\n\nThis project was bootstrapped with \nCreate React App\n.\n\n\nAvailable Scripts\n\n\nIn the project directory, you can run:\n\n\nnpm start\n\n\nRuns the app in the development mode.\\\nOpen \nhttp://localhost:3000\n to view it in your browser.\n\n\nThe page will reload when you make changes.\\\nYou may also see any lint errors in the console.\n\n\nnpm test\n\n\nLaunches the test runner in the interactive watch mode.\\\nSee the section about \nrunning tests\n for more information.\n\n\nnpm run build\n\n\nBuilds the app for production to the \nbuild\n folder.\\\nIt correctly bundles React in production mode and optimizes the build for the best performance.\n\n\nThe build is minified and the filenames include the hashes.\\\nYour app is ready to be deployed!\n\n\nSee the section about \ndeployment\n for more information.\n\n\nnpm run eject\n\n\nNote: this is a one-way operation. Once you \neject\n, you can't go back!\n\n\nIf you aren't satisfied with the build tool and configuration choices, you can \neject\n at any time. This command will remove the single build dependency from your project.\n\n\nInstead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except \neject\n will still work, but they will point to the copied scripts so you can tweak them. At this point you're on your own.\n\n\nYou don't have to ever use \neject\n. The curated feature set is suitable for small and middle deployments, and you shouldn't feel obligated to use this feature. However we understand that this tool wouldn't be useful if you couldn't customize it when you are ready for it.\n\n\nLearn More\n\n\nYou can learn more in the \nCreate React App documentation\n.\n\n\nTo learn React, check out the \nReact documentation\n.\n\n\nCode Splitting\n\n\nThis section has moved here: \nhttps://facebook.github.io/create-react-app/docs/code-splitting\n\n\nAnalyzing the Bundle Size\n\n\nThis section has moved here: \nhttps://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size\n\n\nMaking a Progressive Web App\n\n\nThis section has moved here: \nhttps://facebook.github.io/create-react-app/docs/making-a-progressive-web-app\n\n\nAdvanced Configuration\n\n\nThis section has moved here: \nhttps://facebook.github.io/create-react-app/docs/advanced-configuration\n\n\nDeployment\n\n\nThis section has moved here: \nhttps://facebook.github.io/create-react-app/docs/deployment\n\n\nnpm run build\n fails to minify\n\n\nThis section has moved here: \nhttps://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify"
    },
    {
        "repo_name": "student-info-report",
        "repo_link": "https://github.com/edwin16804/student-info-report",
        "readme": "student-info-report\n\n\nI have designed a project which allows the user to input data such as name, roll no., marks etc. of several students.\nThe user can perform various actions on the data entered such as sort the data and view it roll no. or GRNO. wise.\nThe user can also print the report card of an individual student."
    },
    {
        "repo_name": "Surface-Crack-Detection-CNN-MV-Techniques",
        "repo_link": "https://github.com/edwin16804/Surface-Crack-Detection-CNN-MV-Techniques",
        "readme": "Surface-Crack-Detection-CNN-MV-Techniques"
    },
    {
        "repo_name": "YOLOv10-Navigation-for-the-Visually-Impaired",
        "repo_link": "https://github.com/edwin16804/YOLOv10-Navigation-for-the-Visually-Impaired",
        "readme": "YOLOv10-Navigation-for-the-Visually-Impaired"
    }
]